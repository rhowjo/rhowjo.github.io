<!DOCTYPE HTML>
<!--
	Hyperspace by HTML5 UP
	html5up.net | @ajlkn
	Free for personal and commercial use under the CCA 3.0 license (html5up.net/license)
-->
<html>
	<head>
		<title>Machine Learning</title>
		<meta charset="utf-8" />
		<meta name="viewport" content="width=device-width, initial-scale=1, user-scalable=no" />
		<link rel="stylesheet" href="assets/css/main.css" />
		<noscript><link rel="stylesheet" href="assets/css/noscript.css" /></noscript>
	</head>
	<body class="is-preload">

		<!-- Header -->
			<header id="header">
				<a href="collab4.html" class="title" style="margin-left: 400px">Machine Learning</a>
				<nav>
					<ul>
						<li><a href="index.html">Home</a></li>
						<li><a href="collab4.html" class="active">Machine Learning</a></li>
					</ul>
				</nav>
			</header>
		
		<!-- Sidebar -->
			<section id="sidebar">
				<div class="inner">
					<nav>
						<ul>
							<li><a href="#header">Return to top</a></li>
							<li><a href="#Introduction">Introduction</a></li>
							<li><a href="#Unit 1">Unit 1 - Introduction to Machine Learning</a></li>
							<li><a href="#Unit 2">Unit 2 - Exploratory Data Analysis</a></li>
							<li><a href="#Unit 3">Unit 3 - Correlation and Regression</a></li>
							<li><a href="#Unit 4">Unit 4 - Linear Regression with Scikit-Learn</a></li>
							<li><a href="#Unit 5">Unit 5 - Clustering</a></li>
							<li><a href="#Unit 6">Unit 6 - Clustering with Python</a></li>
							<li><a href="#Unit 7">Unit 7 - Introduction to Artificial Neural Networks (ANNs)</a></li>
							<li><a href="#Unit 8">Unit 8 - Training an Artificial Neural Network</a></li>	
							<li><a href="#Unit 9">Unit 9 - Introduction to Convolutional Neural Networks (CNNs)</a></li>
							<li><a href="#Unit 10">Unit 10 - CNN Interactive Learning</a></li>
							<li><a href="#Unit 11">Unit 11 - Model Selection and Evaluation</a></li>
							<li><a href="#Unit 12">Unit 12 - Industry 4.0 and Machine Learning</a></li>	
							
						</ul>
					</nav>
				</div>
			</section>

		<!-- Wrapper -->
			<div id="wrapper">

				<!-- Main -->
					<section id="Introduction" class="wrapper">
						<div class="inner">
							<h1 class="major">Introduction</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<p>The format of this section of the e-Portfolio for Machine learning is split into sections covering each unit.</p>
							<p>Each unit is split into a general reflection followed by any tasks undertaken during the unit. This means that any collaborative discussion initial and summary posts will appear in the unit they were completed. For example, the initial post for the first collaborative discussion will appear under Unit 1 and the summary post in Unit 3. References will be provided where applicable in each section.</p>						
							<p>Additionally, it should be noted reflections are my general thoughts, feelings, and learnings on the topic of the unit and thus will rarely include any references.</p>
							<p>Tasks may reference and link to jupyter notebooks where all charts and code that are referred to in the text can be found. Please use the links provided as they are key in understanding what the tasks are referencing.</p>
						</div>
							
					</section>
						<section id="Unit 1" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 1 - Introduction to Machine Learning</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This was my second introduction to Machine Learning (as it had been used briefly in another module – Visualising Data).</p>
							<p>I learnt that there are many applications of machine learning in both medical practice and research as well as general business applications. This ranges from using clustering (partitioning data into groups) for customer segmentation to regression models to predict outcomes. Additionally, I learnt about the use of AI and neural networks and some of the risks in terms of legality and ethics. </p>
							<p>On top of this, I started the collaborative discussion regarding the 4th Industrial Revolution which focused on the impact of the failing of an information system.</p>
							<p>After completing this unit, I had a more informed understanding of machine learning in different industries.</p>

							<h2>Task – Collaborative Discussion Initial Post – The 4th Industrial Revolution</h2>
							<p>In September 2020, at the height of the COVID pandemic in the UK, there was an IT failure that meant that nearly 16,000 coronavirus cases were unreported (Kelion, 2020). This was due to Public Health England using an old file format, xls, that could only handle 65,000 rows of data, rather than more modern formats like xlsx than can handle up to 1,000,000 (Vincent, 2020).</p>
							<p>Since this missing data would have been used for the NHS app to alert people that they may have been exposed to the virus, the impact of this meant that thousands of people may have been unaware of this exposure. Subsequently, this could have resulted in increased spread of the virus and further deaths.</p>
							<p>This demonstrates the importance of adapting and adopting new technologies in order to best deliver products and services (Schwab, 2016). There is a real need to replace these outdated systems for something more robust (Sevre et al., 2011). This is especially true when data is critical to public health (Birkhead, 2015).</p>

							<h3>References</h3>
							<p>
								<br>Birkhead, G.S., Klompas, M. and Shah, N.R. (2015) ‘Uses of electronic health records for public health surveillance to Advance Public Health’, Annual Review of Public Health, 36(1), pp. 345–359. doi:10.1146/annurev-publhealth-031914-122747.</br>
								<br>Kelion, L. (2020) Excel: Why using Microsoft’s tool caused covid-19 results to be lost, BBC News. Available at: https://www.bbc.co.uk/news/technology-54423988 (Accessed: 10 August 2023).</br>
								<br>Schwab, K. (2016) The Fourth Industrial Revolution: What It Means and how to respond, World Economic Forum. Available at: https://www.weforum.org/agenda/2016/01/the-fourth-industrial-revolution-what-it-means-and-how-to-respond/ (Accessed: 10 August 2023).</br>
								<br>Sevre, C., dela Cruz, N. and Westgard, T. (2011) ‘Orm and retirement of outdated business system’, On the Move to Meaningful Internet Systems: OTM 2011 Workshops, pp. 258–267. doi:10.1007/978-3-642-25126-9_37.</br>
								<br>Vincent, J. (2020) Excel spreadsheet error blamed for UK’s 16,000 missing coronavirus cases, The Verge. Available at: https://www.theverge.com/2020/10/5/21502141/uk-missing-coronavirus-cases-excel-spreadsheet-error (Accessed: 10 August 2023).</br>
							</p>
						</div>
					</section>
						<section id="Unit 2" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 2 - Exploratory Data Analysis</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This was another topic that I have had experience within a previous module, so naturally the concepts of this felt comfortable for me to learn and grasp.</p>
							<p>This allowed me to refresh my knowledge with EDA. EDA is a process to ensure that the data is understood and can be checked for null values, anomalies, and errors. It allows for appropriate feature selection to then be used in machine learning and can help test hypotheses (Patil, 2018).</p>
							<p>This topic was particularly useful for the first assignment as this was a large part in ensuring an accurate model was possible for the Airbnb dataset. I replaced null values and normalised the data and produced charts to analyse the data to see if there were any patterns.</p>
							
							<h3>References</h2>
							<p>
								<br>Patil, P. (2022) What is Exploratory Data Analysis?, Medium. Available at: https://towardsdatascience.com/exploratory-data-analysis-8fc1cb20fd15 (Accessed: 16 August 2023).</br>

							</p>
							<h2>Task - EDA</h2>
							<p>Please find the Jupyter Notebook used for this task to see code and charts referred in the below text: <li style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Unit%202%20-%20EDA%20task.ipynb">EDA Jupyter Notebook.</a> </li></p>
							<p>In this task, I utilised several libraries such as numpy, pandas, seaborn, and matplotlib to assist in undertaking exploratory data analysis on the auto.mpg dataset downloaded from Kaggle.</p>
							<p>Initially, I looked at the head and tail of the data to get a brief understanding of the different columns and their values (Mahadevan, 2023). I also looked at the overall shape of the dataset to give a definitive answer to how many columns and rows there are (in this case 398 rows and 9 columns).</p>
							<p>An important step in EDA is to check for any errors or anomalies in the data (Chelliah, 2021). One step to check this is to look for null values. Looking at isnull().any() for the dataset, there appear to be no null values. However, when looking at the data types, horsepower is seen as type object. When looking at the values for horsepower, we can see that these are clearly numerical and should be an integer or float.</p>
							<p>Instead of a null value, there is a ‘?’. So, the next step was to change this to a null value, change it to float type and then replace the null value with the median value for the column. There are now no null values in the dataset and all data types are correct.</p>
							<p>Next, the scipy library is imported to look at the skew and kurtosis of the dataset.</p>
							<p>Skewness for columns such as ‘horsepower ‘is greater than 1 meaning that the data is highly skewed (Dan, 2020). Other columns are slightly skewed with some being fairly symmetrical. This level of skewness is generally acceptable.</p>
							<p>The kurtosis measure if the distribution is too peaked. Positive values mean that the distribution peaked more than normal, and a negative kurtosis indicates a flatter shape. Values closer to 0 indicate a dataset that is close to normal distribution. Generally, kurtosis values of +2 and above or -2 and above indicate that either the datasets distribution is too peaked or too flat (Hair et al, 2022). However, in the auto.mpg dataset, this is not the case as values fall within -1.5 to 0.8.</p>
							<p>In the final parts of the EDA, I looked at the correlation between variables. We can see that there are some instances of high correlation, the highest being between weight and displacement with a value of 0.8.</p>
							<p>I decided to explore this correlation further by plotting a scatter graph of the two variables and we can see that although there are outliers, the two variables are correlated.</p>
							<p>There is certainly more that can be done and achieved in an EDA, but this served as a good refresher and served as a good baseline for the EDA performed in the assessment in Unit 6.</p>
							<h3>References</h3>
							<p>
								<br>Chelliah, I. (2021) Data cleaning - how to handle missing values in pandas, Medium. Available at: https://towardsdatascience.com/data-cleaning-how-to-handle-missing-values-in-pandas-cc8570c446ec (Accessed: 16 August 2023).</br>
								<br>Dan, A. (2020) Kurtosis() & skew() function in pandas, Medium. Available at: https://medium.com/@atanudan/kurtosis-skew-function-in-pandas-aa63d72e20de (Accessed: 16 August 2023).</br>
								<br>Hair, J.F. et al. (2022) A primer on partial least squares structural equation modeling (PLS-SEM). Los Angeles i 5 pozostałych: SAGE.</br>
								<br>Mahadevan, M. (2023) Step-by-step exploratory data analysis (EDA) using Python, Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2022/07/step-by-step-exploratory-data-analysis-eda-using-python/ (Accessed: 16 August 2023).</br>

							</p>
						</div>
					</section>
					</section>
						<section id="Unit 3" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 3 - Correlation and Regression</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This is the first unit in the module that started to put theory into practice. In this case, it was looking at the correlation between variables as well as regression.</p>
							<p>I found it useful to learn the differences between correlation and regression and what is best to use in any given situation. More detail on this can be found in the task below.</p>
							<p>Learning more about regression improved my skills and enabled me to create a regression model which was used in the assessment in Unit 6. It was a lot of information to understand but, with time and practice I was able to present working models based on regression.</p>
							<h2>Task – Correlation and Regression</h2>
							<p>To view the charts that are discussed below, please view my jupyter notebook here:<li style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Unit03%20Ex1%20covariance_pearson_correlation(1).ipynb">Correlation Jupyter Notebook</a></li></p>
							<p>In this task I looked at the supplied jupyter notebook and changed the default values to:</p>
							<p style="margin-left: 40px">&bull; data1 = 1 * randn(1000) + 100</p>
							<p style="margin-left: 40px">&bull; data2 = data1 + (10 * randn(1000) + 7)</p>
							<p>It can be observed that any changes to the data points can either negatively or positively affect the correlation. In this first iteration, there is essentially no correlation between data1 and data2. This can be further observed by a Pearsons correlation of 0.116.</p>
							<p>In the second iteration, the correlation increases from first iteration, but is still less than the default values; the second iteration has a Pearson correlation of 0.698 and the default has a Pearsons correlation of 0.888.</p>
							<p>Pearson correlation, which is demonstrated in the charts, is highly sensitive to outliers. An outlier has the capability to highly impact the outcome of the Pearsons correlation value (Magiya, 2022). Statistical methods such as using standard deviation can help alleviate the bias that may be caused by outliers (Dhadse, 2021). Alternative tests such as Spearman’s and Kendall’s are less sensitive to outliers and may be used instead if removing outliers is not an option (Zinda, 2023).</p>
							<p>Correlation, then, is a useful technique for investigating the relationship between variables.</p>
							<p>“Correlation quantifies the strength of the linear relationship between a pair of variables, whereas regression expresses the relationship in the form of an equation.” - Bewick et al., 2003</p>
							<p>Thus, regression measures how two variables affect each other, and correlation measures the relationship. Regression also enables to ability to predict outcomes (Calvello, 2023).</p>
							
							
							<h3>References</h3>
							<p>
								<br>Bewick, V., Cheek, L. and Ball, J. (2003) Critical Care, 7(6), p. 451. doi:10.1186/cc2401</br>
								<br>Calvello, M. (2023) Correlation vs. regression made easy: Which to use and why? - G2, g2.com. Available at: https://www.g2.com/articles/correlation-vs-regression (Accessed: 22 August 2023).</br>
								<br>Dhadse, A. (2021) Removing outliers. understanding how and what behind the magic., Medium. Available at: https://medium.com/analytics-vidhya/removing-outliers-understanding-how-and-what-behind-the-magic-18a78ab480ff (Accessed: 22 August 2023).</br>
								<br>Magiya, J. (2022) Pearson coefficient of correlation explained., Medium. Available at: https://towardsdatascience.com/pearson-coefficient-of-correlation-explained-369991d93404 (Accessed: 22 August 2023).</br>
								<br>Zinda, Z. (2023) Data Science Stats Review: Pearson’s, Kendall’s, and spearman’s correlation for feature selection, phData. Available at: https://www.phdata.io/blog/data-science-stats-review/ (Accessed: 22 August 2023).</br>
							</p>
							<h2>Task – Collaborative Discussion Summary Post – The 4th Industrial Revolution</h2>
							<p>With the increased adoption of technology and rapid increases in the amount of data available, there is a big dependency that has developed in conjunction (Grissinger, 2019). With bigger reliance on big data and technical systems, any errors or failures can cause major issues (Kiess, 2013).</p>
							<p>It can be as simple as failing to be aware of system limitations such as the instance of the UK government being reliant on old file formats to store critical public health data (Vincent, 2020). This failure could have caused increased spread of the coronavirus and subsequently lead to more deaths during the pandemic (Kelion, 2020).</p>
							<p>Another example of reliance, as noted by my peers, is the Kowloon incident where a signal failure results in a 4 hour delay. Officials were unaware of the incident (Outrage at rush-hour rail chaos, 2004).</p>
							<p>This first incident emphasizes the need to ensure that legacy systems are updated (Pratt, 2023). This is especially true when the data is critical to public health.</p>
							<p>The second incident emphasizes the need to ensure that there is human oversight so that when there is a technological failure, human intervention can fix and alleviate the issue (Scanlon, 2022).</p>
							<p>In both cases, a large class of people were affected by these technological failings. Thus, humans must remain diligent and ensure they both understand the technology and its limitations as well as monitor its performance.</p>
							<h3>References</h3>
							<p>
								<br>Grissinger, M. (2019) Understanding human over-reliance on technology, P & T : a peer-reviewed journal for formulary management. Available at: https://www.ncbi.nlm.nih.gov/pmc/articles/PMC6534180/ (Accessed: 22 August 2023).</br>
								<br>Kelion, L. (2020) Excel: Why using Microsoft’s tool caused covid-19 results to be lost, BBC News. Available at: https://www.bbc.co.uk/news/technology-54423988 (Accessed: 10 August 2023).</br>
								<br>Kiess, C. (2013) Errors and adverse consequences as a result of information technology ..., ScholarWorks. Available at: https://scholarworks.iupui.edu/bitstream/handle/1805/3750/Kiess_MasterThesis_Final.pdf (Accessed: 22 August 2023).</br>
								<br>Pratt, M.K. (2023) Replacing vs. maintaining legacy systems: TechTarget, CIO. Available at: https://www.techtarget.com/searchcio/feature/Replacing-vs-maintaining-legacy-systems (Accessed: 22 August 2023).</br>
								<br>Scanlon, L. (2022) What meaningful human oversight of AI should look like, Pinsent Masons. Available at: https://www.pinsentmasons.com/out-law/analysis/what-meaningful-human-oversight-of-ai-should-look-like (Accessed: 22 August 2023).</br>
								<br>Vincent, J. (2020) Excel spreadsheet error blamed for UK’s 16,000 missing coronavirus cases, The Verge. Available at: https://www.theverge.com/2020/10/5/21502141/uk-missing-coronavirus-cases-excel-spreadsheet-error (Accessed: 10 August 2023).</br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 4" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 4 - Linear Regression with Scikit-Learn</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This unit helped me build upon my knowledge of linear regression from the previous units and introduced me to the Scikit-Learn library in python. This enabled me to easily create my own linear regression models. </p>
							<p>However, in this unit, I struggled with this task and sought the help from classmates and conducted research to help find answers. In the end, I managed to come up with a working model for correlation and regression for the GDP and Population datasets. I utilised my skills gained from the EDA unit to help with the initial data preprocessing but needed help on combining the data as this was not something I had previous experience in.</p>
							<h2>Task – Correlation and Regression</h2>
							<p>Please see the Jupyter Notebook for the task here: <li style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Unit%204%20-%20Task.ipynb">GDP and Population Jupyter Notebook</a></li></p>
							<p>In this task, we were given two datasets. The first dataset contained data for the GDP of each country and the second dataset contained data for the population of each country. We were tasked to combine the datasets to create a correlation graph that showed the relationship between the mean population and mean GDP of all the countries from 2001 to 2020.</p>
							<p>The data firstly had to be pre-processed so that the there were no null values by replacing them with the median of any given column. Then, the data was limited to 2001 onwards and converted to numeric. After this, the mean for both datasets were calculated and put into a single data frame.</p>
							<p>When showing this in a scatter plot, we can see that correlation is sparse. A Pearsons correlation coefficient of 0.031 confirms this. There are a quite a few outliers which can also affect the coefficient score.</p>
							<p>This data was then put into a linear regression model. Firstly, the data was reshaped into one column and then split into train and test datasets with a test size of 25%.</p>
							<p>After training, we can see that the R2 score is 0.0019, generally meaning that 0.19% of the variance of the dependent variable is explained by the independent variable. That is to say that this is not a good model for predicting GDP from population or population from GDP. </p>
						
						</div>
					</section>
					</section>
						<section id="Unit 5" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 5 - Clustering</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This unit focused on clustering. Clustering is a method of partitioning data into groups (clusters). These are defined as data points that have more similarity that other data points. It’s predominately used for customer segmentation in businesses (Kaushik, 2023).</p>
							<p>This was my first experience with clustering but it’s applications and uses made sense to me from a business perspective, especially when envisioning customer segmentation. With this knowledge, I was more comfortable going into future units because I had a good base understanding in terms of how the measurements of distance worked as well as how to evaluate whether clustering has worked well or not (Manimaran, 2021).</p>
							<h3>References</h3>
							<p>
								<br>Kaushik, S. (2023) Clustering: Introduction, different methods, and applications (updated 2023), Analytics Vidhya. Available at: https://www.analyticsvidhya.com/blog/2016/11/an-introduction-to-clustering-and-different-methods-of-clustering/ (Accessed: 05 September 2023).</br>
								<br>Manimaran (2021) Clustering evaluation strategies, Medium. Available at: https://towardsdatascience.com/clustering-evaluation-strategies-98a4006fcfc (Accessed: 07 September 2023).</br>
							</p>
							<h2>Task – Jaccard Coefficient</h2>
							<p>For this task, we were asked to look at Jaccard Coefficient Calculations also known as Jaccard similarity. This is used to compute the similarity between objects.</p>
							<p>The formula is thus: JC=Ni/(Na+Nb−Ni)</p>
							<p style="margin-left: 40px">&bull; Ni is the intersecting elements (how many elements are shared between the sets</p>
							<p style="margin-left: 40px">&bull; Na is the number of elements in set A.</p>
							<p style="margin-left: 40px">&bull; Nb is the number of elements in set B.</p>
							<p>In this scenario, elements will be instances of Y and P in the various columns.</p>
							<p>Jack and Mary:</p>
							<p style="margin-left: 40px">&bull; Jack – Fever, Test-1 = 2 elements</p>
							<p style="margin-left: 40px">&bull; Mary – Fever, Test-1, Test-3 = 3 elements</p>
							<p style="margin-left: 40px">&bull; Intersecting Elements = 2</p>
							<p style="margin-left: 40px">&bull; JC = 2/ (2+3−2) = 0.66</p>
							<p>Jack and Jim:</p>
							<p style="margin-left: 40px">&bull; Jack – Fever, Test-1 = 2 elements</p>
							<p style="margin-left: 40px">&bull; Jim – Fever, Cough = 2 elements</p>
							<p style="margin-left: 40px">&bull; Intersecting Elements = 1</p>
							<p style="margin-left: 40px">&bull; JC = 1/ (2+2-1) = 0.33</p>
							<p>Jim and Mary</p>
							<p style="margin-left: 40px">&bull; Jim – Fever, Cough = 2 elements</p>
							<p style="margin-left: 40px">&bull; Mary – Fever, Test-1, Test-3 = 3 elements</p>
							<p style="margin-left: 40px">&bull; Intersecting Elements = 1</p>
							<p style="margin-left: 40px">&bull; JC = 1/ (2+3-1) = 0.25</p>
						</div>
					</section>
					</section>
						<section id="Unit 6" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 6 - Clustering with Python</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This reflection will focus on the group project as in this Unit, the first group project was due. This was a difficult process as the team was not working well. Each person did their own EDA and subsequently created their own models. However, too much focus was put on k-means clustering which didn’t produce successful results on this dataset. It was my idea to use regression models to predict price. </p>
							<p>We didn’t settle on a clear question until after the EDA. After this analysis, we produced the question: “what features have the biggest impact on price?”</p>
							<p>In my version, which produced the final model, the model was based of regression methods and iterated through different models such as linear regression, decision tree, random forest, and XGBoost.</p>
							<p>It was in this process that I learnt about hyperparameter tuning in order to prevent overfitting, as well as increasing accuracy (Olaoye, 2022). In the end, I produced a model that reached 64% accuracy, which, with more relevant variables included in the data, could be even higher (T, 2019).</p>
							<p>This was a difficult task to complete because I had essentially done all the code and the final report on my own with little outside help from my teammates.</p>
							<p>It was beneficial that I am self-proficient and had a good understanding of what needed to be done to complete this task. Although, I do believe that with better collaboration between the group, we would have gotten a better grade.</p>
							<h3>References</h3>
							<p>
								<br>Olaoye, A. (2022) 4 effective ways to prevent overfitting and why they work, Medium. Available at: https://towardsdatascience.com/4-effective-ways-to-prevent-overfitting-and-why-they-work-f6e3b98aefda (Accessed: 13 September 2023). </br>
								<br>T, S.B. (2019) Having more features leads to a better prediction, yes/no?, Medium. Available at: https://medium.datadriveninvestor.com/having-more-features-leads-to-a-better-prediction-yes-no-f82cc6183b37 (Accessed: 13 September 2023).</br>

							</p>
							<h2>Task – K-means Clustering</h2>
							<h3>Task A: Iris</h3>
							<p>Link to Jupyter Notebook for Task A: <li  style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Task%20A%20-%20Iris.ipynb">Task A Jupyter Notebook</a></li></p>
							<p>Firstly, k-mean clustering was performed on the iris dataset.</p>
							<p>To begin, the distributions between each variable was looked at. We can already see that the different species are already grouped in some ways when it comes to relationships between variables.</p>
							<p>Next, k-means cluster was performed for 1-10 clusters. Using the elbow curve, we can see the optimal number of clusters is 3 – which matches the number of species in the dataset.</p>
							<p>We can see that the clusters are mostly distinct but with some overlap.</p>
							<h3>Task B: Wine</h3>
							<p>Link to Jupyter Notebook for Task B: <li style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Task%20B%20-%20Wine.ipynb">Task B Jupyter Notebook</a></li></p>
							<p>The second task was to look at the wine data set. Similar to the Iris dataset, there are 3 unique values for wine. The process for k-means clustering was comparable to the Iris dataset. When looking at the elbow curve, again, the optimal number of clusters is 3.</p>
							<p>However, for this dataset I used PCA (principal component analysis) to help plot the data. This reduces the dimensionality of the data set. The idea is to reduce the number of variables whilst preserving as much information as possible (Jaadi, 2023).</p>
							<p>In the plot we can see three distinct clusters for wine and their respective centroids. Following this is a heat map of the variables.</p>
							<h3>Task C: Weather</h3>
							<p>Link to Jupyter Notebook for Task C: <li style="margin-left: 40px"><a href="https://github.com/rhowjo/rhowjo.github.io/blob/main/assets/Task%20C%20-%20Weather.ipynb">Task C Jupyter Notebook</a></li></p>
							<p>For the final task, I used the same methods as in the other two datasets and plotted an elbow curve, which again has optimal number of clusters of 3. However, after looping through we can see the centroid are very close together for each group and there is an increasing overlap/overfit as the clusters increase.</p>
							<h3>References</h3>
							<p>
								<br>Jaadi, Z. (2023) A step-by-step explanation of principal component analysis (PCA), Built In. Available at: https://builtin.com/data-science/step-step-explanation-principal-component-analysis (Accessed: 14 September 2023).</br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 7" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 7 - Introduction to Artificial Neural Networks (ANNs)</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This was my first experience of neural networks. This introduction to ANN helped immensely when creating an initial model for the unit 11 project to create a neural network using the CIFAR-10 dataset. It provided me a fundamental understanding and baseline to work from.</p>
							<p>The fact that artificial neural networks are based on the human brain in terms of structure is incredibly interesting to me and helped me develop a better understanding by examining this analogy (Walczak & Cerpa, 2003).</p>
							<h3>References</h3>
							<p>
								<br>Walczak, S. and Cerpa, N. (2003) ‘Artificial Neural Networks’, Encyclopedia of Physical Science and Technology, pp. 631–645. doi:10.1016/b0-12-227410-5/00837-1.</br>
							</p>
							<h2>Task - Perceptron Activities</h2>
							<p>In this task I gained a deeper understanding of Perceptron. This is a neural network link containing computations that track features and uses AI in the input data. In other words, it’s an ANN.</p>
							<p>It is an algorithm for supervised learning of binary classifiers. It enables neurons to learn process elements in the training set one at a time (Banoula, 2023).</p>
							<p>Below are observations based on the Jupyter Notebooks provided in the seminar materials:</p>
							<p style="margin-left: 40px">&bull; In exercise 1, by changing the inputs and weights, I observed different results. For example, having positive weight i.e., 04, 02 resulted in the sum function being greater than or equal to 1. This means that the neuron in this case is activated.</p>
							<p style="margin-left: 40px">&bull; In exercise 2, the step activation function is used again. This time the small dataset it then trained to then be able to classify new instances of data. This helped me understand the basics of how an ANN works and how it can be used to classify new data after training the model on a very small dataset</p>
							<p style="margin-left: 40px">&bull; In exercise 3, the sigmoid function is used to create a multi-layer Perceptron. This time, epochs are introduced. The number of epochs defines the number of times the algorithm will work through the training dataset. In this exercise, 100 epochs are used and then up to 1,000,000. This shows a complete neural network and demonstrates how it performs.</p>
							<h3>References</h3>
							<p>
								<br>Banoula, M. (2023) What is Perceptron? A beginners guide for 2023: Simplilearn, Simplilearn.com. Available at: https://www.simplilearn.com/tutorials/deep-learning-tutorial/perceptron (Accessed: 21 September 2023).</br>
							</p>
							
						</div>
					</section>
					</section>
						<section id="Unit 8" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 8 - Training an Artificial Neural Network</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<h2>Reflection</h2>
							<p>This unit formed part of my biggest contribution to the final group project. It covered the compliance and regulatory frameworks for managing data. This includes regulations (such as GDPR (Regulation (EU) 2016/679 (General Data Protection Regulation), 2016)) and standards (such as ISO27002). These essentially form a basis that organisation should follow for compliance (and by being compliant, increase chances to be more secure) (Kwon & Johnson, 2011).</p>
							<h2>Bibliography</h2>
							<p>
								<br>Kwon, J. and Johnson, E. (2011) “The Impact of Security Practices on Regulatory Compliance and Security Performance,” International Conference on Information Systems [Preprint].</br>
								<br>‘Regulation (EU) 2016/679 of the European Parliament and the Council of 27 April 2016 on the protection of natural persons with regard to the processing of personal data and the on the free movement of such data, and repealing Directive 95/46/EC’ (2016), Official Journal of the European Union L 119, pp. 1-88.</br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 9" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 9 - Database Management Systems (DBMS) and Models</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<p>DBMS is software used for creating and managing databases. It allows users to be able to update data, delete it, create it etc. Database management systems such as MySQL and Oracle DBMS for structured relational data make data organised and accessible (Mullins, 2022).</p>							
							<h2>Bibliography</h2>
							<p>
								<br>Mullins, C.S. (2022) Database management system (DBMS), Data Management. TechTarget. Available at: https://www.techtarget.com/searchdatamanagement/definition/database-management-system (Accessed: April 12, 2023). </br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 10" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 10 - More on APIs (Application Programming Interfaces) for Data Parsing</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<p>In this unit I worked in our team to produce a document on API security requirements. See here <li><a href="https://github.com/rhowjo/rhowjo.github.io/blob/f2d46afb901f8486aa2d75b911e78dc394533412/api_security_brief%20(1).pdf">Here</a></p>
							<p>I learned more about APIs (application programming interface) which essentially enables two programs to communicate (Lutkevich & Nolle, 2022). A application such as Facebook has a marketing API the users can connect with their application using the Facebook Marketing API. This enables access to data upon request (Marketing API).</p>
							<h2>Bibliography</h2>
							<p>
								<br>Lutkevich, B. and Nolle, T. (2022) What is an API (application programming interface)?, App Architecture. TechTarget. Available at: https://www.techtarget.com/searchapparchitecture/definition/application-program-interface-API (Accessed: April 12, 2023).</br>
								<br>Marketing API (no date) Meta Marketing API - Documentation - Meta for Developers. Available at: https://developers.facebook.com/docs/marketing-apis/ (Accessed: April 12, 2023). </br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 11" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 11 - DBMS Transaction and Recovery</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<p>In this unit I learned about various backup strategies. One such strategy is the Grandfather-Father-Son (GFS) backup strategy which is a backup rotation scheme. It consists of three or more backup cycles (daily, weekly, and monthly) allowing for independent retention policies for all (K, 2017).</p>
							<p>There are three tiers of backup within this strategy: the grandfather which is a full machine backup (usually offsite), the father which is another full backup to a local device, and the son is a regular incremental backup (Northcott, 2021).</p>
							<h2>Bibliography</h2>
							<p>
								<br>K, A. (2017) Sneak Peek of grandfather-father-son backup in MSP360 backup, MSP360 Blog. Available at: https://www.msp360.com/resources/blog/sneak-peek-of-grandfather-father-son-backup-in-backup/ (Accessed: April 12, 2023).</br>
								<br>Northcott, J. (2021) Backup best practices - the grandfather-father-son backup strategy, Vitanium. Available at: https://vitanium.com/best-backup-practices-the-grandfather-father-son-backup-strategy/ (Accessed: April 12, 2023). .</br>
							</p>
						</div>
					</section>
					</section>
						<section id="Unit 12" class="wrapper">
						<div class="inner">
							<h1 class="major">Unit 12 - Future of Big Data Analytics</h1>
							<!-- <span class="image fit"><img src="images/pic04.jpg" alt="" /></span> -->
							<p>The future of big data analytics is a big topic, and with increased volumes of data, industries need to be set to scale with this rapidly increasing volume in terms of analytical capabilities (The Future of Big Data Analytics & Data Science: 5 trends of Tomorrow, 2023).</p>
							<p>Khvoynitskaya also believes data volumes will increases and that more businesses will migrate to the cloud (Khvoynitskaya, 2020).</p>
							<p>I believe that with this increase in data volume, will increase the demand for data scientists and understanding of underlying data concepts.</p>
							<h2>Bibliography</h2>
							<p>
								<br>Khvoynitskaya, S. (2020) The Future of BIG DATA: 5 predictions from experts for 2020-2025, Itransition. Available at: https://www.itransition.com/blog/the-future-of-big-data (Accessed: April 12, 2023).</br>
								<br>The Future of Big Data Analytics & Data Science: 5 trends of Tomorrow (2023) Monte Carlo Data. Available at: https://www.montecarlodata.com/blog-the-future-of-big-data-analytics-and-data-science/ (Accessed: April 12, 2023).</br>
							</p>
						</div>
					</section>
					
			</div>

		<!-- Footer -->
			<footer id="footer" class="wrapper alt">
				<div class="inner">
					<ul class="menu">
						<li>&copy; Rhiannon. All rights reserved.</li><li>Design: <a href="http://html5up.net">HTML5 UP</a></li>
					</ul>
				</div>
			</footer>

		<!-- Scripts -->
			<script src="assets/js/jquery.min.js"></script>
			<script src="assets/js/jquery.scrollex.min.js"></script>
			<script src="assets/js/jquery.scrolly.min.js"></script>
			<script src="assets/js/browser.min.js"></script>
			<script src="assets/js/breakpoints.min.js"></script>
			<script src="assets/js/util.js"></script>
			<script src="assets/js/main.js"></script>

	</body>
</html>
